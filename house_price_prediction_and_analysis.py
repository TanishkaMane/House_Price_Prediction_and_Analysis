# -*- coding: utf-8 -*-
"""House Price Prediction and Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rxT5tiasK5MxJHYgFOP2Av3xN8Kl3qIJ

*************************************************************
# Data Preprocessing
*************************************************************

1.1 Load the "house-prices.csv" dataset into a Pandas DataFrame:
"""

import pandas as pd

# Loading the CSV file into a DataFrame
df = pd.read_csv('house_prices.csv')

"""1.2 Perform data cleaning, including handling missing values, outliers, and duplicates:"""

#  check for missing values in the DataFrame
df.isnull().sum()
# Check for and remove duplicate rows using
df.drop_duplicates()

""" 1.3 Encode categorical variables using one-hot encoding or label encoding:"""

df = pd.get_dummies(df, columns=['Neighborhood', 'Brick'], drop_first=True)
print(df.shape)
print(df.head())
real_data = df.copy()

"""1.4 Handle numerical features, such as scaling or transformation:"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[["SqFt", "Bedrooms", "Bathrooms", "Offers"]] = scaler.fit_transform(df[["SqFt", "Bedrooms", "Bathrooms", "Offers"]])

"""1.5 Split the dataset into training and testing sets:"""

from sklearn.model_selection import train_test_split

X = df.drop("Price", axis=1)  # Features
y = df["Price"]              # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""*************************************************************
# Exploratory Data Analysis (EDA)
*************************************************************

2.1 Explore the dataset's structure and statistics:
"""

# Explore dataset structure
print(df.info())

# Summary statistics
print(df.describe())

"""2.2 Visualize the distribution of house prices using a histogram:"""

import matplotlib.pyplot as plt

plt.hist(df['Price'], bins=20, edgecolor='k')
plt.xlabel('House Price')
plt.ylabel('Frequency')
plt.title('Distribution of House Prices')
plt.show()

"""2.3 Investigate the relationships between features and house prices:"""

# Scatter plot for a numerical feature (e.g., SqFt vs. Price)
plt.scatter(df['SqFt'], df['Price'])
plt.xlabel('SqFt')
plt.ylabel('House Price')
plt.title('SqFt vs. House Price')
plt.show()

# Correlation matrix
correlation_matrix = df.corr()
print(correlation_matrix)

"""2.4 Identify the most important features that may affect house prices:"""

# Sort features by absolute correlation with Price
correlation_with_price = correlation_matrix['Price'].abs().sort_values(ascending=False)
print(correlation_with_price)

"""2.5 Provide insights and observations based on your EDA:"""

correlations_with_target = correlation_matrix['Price'].sort_values(ascending=False)
print("Correlations with HousePrice:")
print(correlations_with_target[:])

"""*************************************************************
# Model Building
*************************************************************

3.1 Selecting three different machine learning algorithms suitable for regression:
"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

"""3.2 Train and evaluate each model using appropriate evaluation metrics:"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize the models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest": RandomForestRegressor()
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Evaluate the model
    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    r2 = r2_score(y_test, y_pred)

    print(f"{name} Metrics:")
    print(f"Mean Absolute Error: {mae}")
    print(f"Root Mean Squared Error: {rmse}")
    print(f"R-squared: {r2}")
    print("\n")

"""3.3 Implement hyperparameter tuning for one of the models:"""

from sklearn.model_selection import GridSearchCV

# Define hyperparameters to search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a GridSearchCV object
grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, n_jobs=-1)

# Fit the GridSearchCV object to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Evaluate the model with the best hyperparameters
best_rf_model = grid_search.best_estimator_
y_pred_best = best_rf_model.predict(X_test)

"""3.5 Visualize the predictions of the selected model against the actual house prices:"""

# Visualize the predictions of the selected model
plt.scatter(y_test, y_pred_best)
plt.xlabel('Actual House Prices')
plt.ylabel('Predicted House Prices')
plt.title('Actual vs. Predicted House Prices (Random Forest)')
plt.show()

"""*************************************************************
# Model Interpretability
*************************************************************

4.1 Use model interpretability techniques:
Feature Importance (for Random Forest):
"""

# Get feature importances from the Random Forest model
feature_importances = best_rf_model.feature_importances_

# Create a DataFrame to visualize feature importances
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance (Random Forest)')
plt.show()

"""4.2 observing the above visualisation the insights are that according to the model, features like neighnorhood_west and Sqft have the most impact on the houseprices."""